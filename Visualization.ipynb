{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%autoreload 2"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "id": "U5SXMcYAarz5"
   },
   "source": [
    "# Visualization of RICE-N and RL training\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "id": "ukp1MeR1Q0dG"
   },
   "source": [
    "## Load dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-06-28T00:05:15.112923Z",
     "start_time": "2022-06-28T00:05:14.814685Z"
    },
    "id": "u1kP2TiblHSv"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "_ROOT = os.getcwd()\n",
    "sys.path.append(_ROOT+\"/scripts\")\n",
    "sys.path = [os.path.join(_ROOT, \"/scripts\")] + sys.path\n",
    "\n",
    "from desired_outputs import desired_outputs\n",
    "from importlib import reload\n",
    "from codecarbon import EmissionsTracker"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "id": "hrqSp18wlHS2"
   },
   "source": [
    "## Train agents with CPU"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "mq3kgyo9lHS2"
   },
   "source": [
    "CPU-based training can also be done with `rllib`, although it can take much longer depending on the complexity of the negotiation protocol (~3 times longer for the naive negotiation protocol)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "vQzgkrwtlHS3"
   },
   "outputs": [],
   "source": [
    "# This is necessary for rllib to get the correct path!\n",
    "os.chdir(_ROOT+\"/scripts\")\n",
    "import train_with_rllib as cpu_trainer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "kkmFGiccQUUP"
   },
   "source": [
    "Here are some suggested baseline parameter values. The training process is done by a single CPU.\n",
    "\n",
    "```python\n",
    "num_envs = 1 # ensemble results with 100 random intialized enviornments\n",
    "train_batch_size = 1024 # train with 1024 batch_size\n",
    "num_episodes = 30000 # number of episodes\n",
    "lr = 0.005 # learning rate\n",
    "model_params_save_freq = 5000 # save model for every 5000 steps\n",
    "num_workers=1 # a single CPU\n",
    "```\n",
    "Additionally, we specify \n",
    "```python \n",
    "negotiation_on = 0 # no negotiation\n",
    "```\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "mjRpvEmoqLS2"
   },
   "source": [
    "Running this next cell will take ~6 minutes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cpu_trainer = reload(cpu_trainer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "p4cen253lHS3",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "cpu_trainer_off, cpu_nego_off_ts = cpu_trainer.trainer(negotiation_on=0,  # no negotiation\n",
    "  num_envs=1, \n",
    "  train_batch_size=1024, \n",
    "  num_episodes=10, \n",
    "  lr=0.0003, \n",
    "  model_params_save_freq=5000, \n",
    "  desired_outputs=desired_outputs, # a list of values that the simulator will output\n",
    "  num_workers=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "j1mlWgecQZ6s"
   },
   "source": [
    "To train the agents with negotiation, we modify ``negotiation_on``:\n",
    "\n",
    "```python\n",
    "negotiation_on = 1 # with naive negotiation\n",
    "```\n",
    "A naive negotiation protocol is already implemented, but **participants are expected to modify, improve and/or replace this protocol to maximize climate and economic outcomes**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "UN1EHJtVp-co"
   },
   "source": [
    "Running this next cell will take  ~33 minutes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cpu_trainer = reload(cpu_trainer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "egkatPyElHS3"
   },
   "outputs": [],
   "source": [
    "cpu_trainer_on, cpu_nego_on_ts = cpu_trainer.trainer(negotiation_on=1, # with naive negotiation\n",
    "  num_envs=1, \n",
    "  train_batch_size=1024, \n",
    "  num_episodes=300, \n",
    "  lr=0.0005, \n",
    "  model_params_save_freq=5000, \n",
    "  desired_outputs=desired_outputs, # a list of values that the simulator will output\n",
    "  num_workers=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "7q_PO6bilqCU"
   },
   "source": [
    "The trainer `cpu_trainer_on` closes gracefully, so `cpu_nego_on_ts` contains the timeseries data from the trainer.\n",
    "\n",
    "If the process is killed during training, reducing ``num_envs`` and ``train_batch_size`` can help to some extent."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "OF4mXVOHlHS3"
   },
   "source": [
    "# Save or load from previous training results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "80g85HbZlHS4"
   },
   "source": [
    "This section is for saving and loading the results of training (not the trainer itself)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "K9zxtDGzlHS4"
   },
   "outputs": [],
   "source": [
    "from opt_helper import save, load"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "03fOZB9fpHAs"
   },
   "source": [
    "To save the output timeseries: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "lYF6UDHKlHS4"
   },
   "outputs": [],
   "source": [
    "# [uncomment below to save]\n",
    "# save({\"nego_off\":gpu_nego_off_ts, \"nego_on\":gpu_nego_on_ts}, \"filename.pkl\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "0vG1JZ75pIa7"
   },
   "source": [
    "To load the output timeseries:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "4TEE7CvHlHS4"
   },
   "outputs": [],
   "source": [
    "# [uncomment below to load]\n",
    "# dict_ts = load(\"filename.pkl\")\n",
    "# nego_off_ts, nego_on_ts = dict_ts[\"nego_off\"], dict_ts[\"nego_on\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Plot training procedures"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "One may want to plot the some metrics such as `mean reward` which are logged during the training procedure.\n",
    "\n",
    "```python\n",
    "metrics = ['Iterations Completed',\n",
    " 'VF loss coefficient',\n",
    " 'Entropy coefficient',\n",
    " 'Total loss',\n",
    " 'Policy loss',\n",
    " 'Value function loss',\n",
    " 'Mean rewards',\n",
    " 'Max. rewards',\n",
    " 'Min. rewards',\n",
    " 'Mean value function',\n",
    " 'Mean advantages',\n",
    " 'Mean (norm.) advantages',\n",
    " 'Mean (discounted) returns',\n",
    " 'Mean normalized returns',\n",
    " 'Mean entropy',\n",
    " 'Variance explained by the value function',\n",
    " 'Gradient norm',\n",
    " 'Learning rate',\n",
    " 'Mean episodic reward',\n",
    " 'Mean policy eval time per iter (ms)',\n",
    " 'Mean action sample time per iter (ms)',\n",
    " 'Mean env. step time per iter (ms)',\n",
    " 'Mean training time per iter (ms)',\n",
    " 'Mean total time per iter (ms)',\n",
    " 'Mean steps per sec (policy eval)',\n",
    " 'Mean steps per sec (action sample)',\n",
    " 'Mean steps per sec (env. step)',\n",
    " 'Mean steps per sec (training time)',\n",
    " 'Mean steps per sec (total)'\n",
    " ]\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To check out the logged submissions, please run the following block."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from glob import glob\n",
    "glob(os.path.join(_ROOT,\"Submissions/*.zip\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If previous trainings are finished and logged properly, this should give a list of `*.zip` files where the logs are included. \n",
    "\n",
    "We picked one of the submissions and the metric `Mean episodic reward` as an example, please check the code below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from opt_helper import get_training_curve, plot_training_curve\n",
    "\n",
    "log_zip = glob(os.path.join(_ROOT,\"Submissions/*.zip\"))[0]\n",
    "plot_training_curve(None, 'Mean episodic reward', log_zip)\n",
    "\n",
    "# to check the raw logging dictionary, uncomment below\n",
    "# logs = get_training_curve(log_zip)\n",
    "# logs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "0BCG5IYWlHS5"
   },
   "source": [
    "# Plot results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "oZW6-QJGlHS5"
   },
   "outputs": [],
   "source": [
    "from desired_outputs import desired_outputs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "AdxL0JanlHS5"
   },
   "source": [
    "One may want to check the performance of the agents by plotting graphs. Below, we list all the logged variables. One may change the ``desired_outputs.py`` to add more variables of interest.\n",
    "\n",
    "```python\n",
    "desired_outputs = ['global_temperature', \n",
    "  'global_carbon_mass', \n",
    "  'capital_all_regions', \n",
    "  'labor_all_regions', \n",
    "  'production_factor_all_regions', \n",
    "  'intensity_all_regions', \n",
    "  'global_exogenous_emissions', \n",
    "  'global_land_emissions', \n",
    "  'timestep', \n",
    "  'activity_timestep', \n",
    "  'capital_depreciation_all_regions', \n",
    "  'savings_all_regions', \n",
    "  'mitigation_rate_all_regions', \n",
    "  'max_export_limit_all_regions', \n",
    "  'mitigation_cost_all_regions', \n",
    "  'damages_all_regions', \n",
    "  'abatement_cost_all_regions', \n",
    "  'utility_all_regions', \n",
    "  'social_welfare_all_regions', \n",
    "  'reward_all_regions', \n",
    "  'consumption_all_regions', \n",
    "  'current_balance_all_regions', \n",
    "  'gross_output_all_regions', \n",
    "  'investment_all_regions', \n",
    "  'production_all_regions', \n",
    "  'tariffs', \n",
    "  'future_tariffs', \n",
    "  'scaled_imports', \n",
    "  'desired_imports', \n",
    "  'tariffed_imports', \n",
    "  'stage', \n",
    "  'minimum_mitigation_rate_all_regions', \n",
    "  'promised_mitigation_rate', \n",
    "  'requested_mitigation_rate', \n",
    "  'proposal_decisions',\n",
    "  'global_consumption',\n",
    "  'global_production']\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "rYLsNHRjlHS5"
   },
   "outputs": [],
   "source": [
    "from opt_helper import plot_result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-Ab3Dd-zlHS5"
   },
   "source": [
    "`plot_result()` plots the time series of logged variables.\n",
    "\n",
    "```python\n",
    "plot_result(variables, nego_off, nego_on, k)\n",
    "```\n",
    "* ``variables`` can be either a single variable of interest or a list of variable names from the above list. \n",
    "* The ``nego_off_ts`` and ``nego_on_ts`` are the logged time series for these variables, with and without negotiation. \n",
    "* ``k`` represents the dimension of the variable of interest ( it should be ``0`` by default for most situations)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "mrn13h2V2jBQ"
   },
   "source": [
    "Here's an example of plotting a single variable of interest."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "nDdreoz2lHS6"
   },
   "outputs": [],
   "source": [
    "plot_result(\"global_temperature\", \n",
    "  nego_off=gpu_nego_off_ts, # change it to cpu_nego_off_ts if using CPU\n",
    "  nego_on=gpu_nego_on_ts, \n",
    "  k=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "VNXPz9kk2meL"
   },
   "source": [
    "Here's an example of plotting a list of variables."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Z76yOBp3lHS5",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "plot_result(desired_outputs[0:3], # truncated for demonstration purposes\n",
    "  nego_off=gpu_nego_off_ts, \n",
    "  nego_on=gpu_nego_on_ts, \n",
    "  k=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "6sJSQ5gdxCni"
   },
   "source": [
    "If one only want to plot negotiation-off plots, feel free to set `nego_on=None`. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "FkuU8kV2xCnj"
   },
   "outputs": [],
   "source": [
    "plot_result(desired_outputs[0:3], # truncated for demonstration purposes\n",
    "  nego_off=gpu_nego_off_ts, \n",
    "  nego_on=None, \n",
    "  k=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "yDI4p7cqlHS6"
   },
   "source": [
    "# How to quickly evaluate the results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "6TSbZcQzlHS6"
   },
   "source": [
    "This section to for evaluating the trained agents. One can edit the evaluation function ``eval metrics`` in ``evaluate_submission.py`` to include more metrics of interest.\n",
    "\n",
    "The evaluation script requires as input:\n",
    "1. The trainer\n",
    "2. The logged_variables\n",
    "3. The framework of the trainer. If using GPU-based training, it should be ``warpdrive``. If using CPU-based training, it should be ``rllib``.\n",
    "\n",
    "We give one example below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "NCk6omCbx9GY"
   },
   "outputs": [],
   "source": [
    "os.chdir(os.path.join(_ROOT,\"scripts\"))\n",
    "from evaluate_submission import val_metrics\n",
    "val_metrics(trainer=gpu_trainer_off, logged_ts=gpu_nego_off_ts, framework=\"warpdrive\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "LuF76W4FlHS6"
   },
   "source": [
    "# Code pieces that can be modified"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "pXdnxXJ7m9Va"
   },
   "source": [
    "As a running example, we use the bilateral negotiation protocol. For more examples, please see section 5.3 in [the white paper](https://github.com/mila-iqia/climate-cooperation-competition/blob/main/White_Paper.pdf)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "mu48DrgvlHS6"
   },
   "source": [
    "## Introduction of environment codes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "nOO9JTyHlHS6"
   },
   "source": [
    "[``rice.py``](https://github.com/mila-iqia/climate-cooperation-competition/blob/main/rice.py), [``rice_cuda.py``](https://github.com/mila-iqia/climate-cooperation-competition/blob/main/rice_cuda.py), [``rice_step.cu``](https://github.com/mila-iqia/climate-cooperation-competition/blob/main/rice_step.cu) and [``rice_helpers.py``](https://github.com/mila-iqia/climate-cooperation-competition/blob/main/rice_helpers.py) are responsible for the GPU code.\n",
    "\n",
    "* [``rice.py``](https://github.com/mila-iqia/climate-cooperation-competition/blob/main/rice.py) includes interactions between the agents and the environment. **[``rice.py``](https://github.com/mila-iqia/climate-cooperation-competition/blob/main/rice.py) is the main script to be modified.**\n",
    "\n",
    "* [``rice_helpers.py``](https://github.com/mila-iqia/climate-cooperation-competition/blob/main/rice_helpers.py) includes all the socioeconomic and climate dynamics. [``rice_helpers.py``](https://github.com/mila-iqia/climate-cooperation-competition/blob/main/rice_helpers.py) should not be changed.\n",
    "\n",
    "* [GPU needed] [``rice_cuda.py``](https://github.com/mila-iqia/climate-cooperation-competition/blob/main/rice_cuda.py) connects the data between the python script and CUDA code.\n",
    "\n",
    "* [GPU needed] [``rice_step.cu``](https://github.com/mila-iqia/climate-cooperation-competition/blob/main/rice_step.cu) is the CUDA version of the code which contains the socioeconomic and climate dynamics, as well as the interactions between the agents and the environment. **To use GPU-based training, the CUDA code in ``rice_step.cu`` must have the same logic as the python code in [``rice.py``](https://github.com/mila-iqia/climate-cooperation-competition/blob/main/rice.py) and [``rice_helpers.py``](https://github.com/mila-iqia/climate-cooperation-competition/blob/main/rice_helpers.py).** The CUDA code mostly follows the grammar of C++. Please refer to [here](https://docs.nvidia.com/cuda/cuda-c-programming-guide/index.html) for more details.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "wERUqHqJlHS7"
   },
   "source": [
    "## How to add extra observations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "OmERwCAjlHS7"
   },
   "source": [
    "To add extra observations or make changes to the observation space, at least two functions must be modified.\n",
    "1.   [`generate_observation()`](https://github.com/mila-iqia/climate-cooperation-competition/blob/main/rice.py#L379)\n",
    "2.   [`reset()`](https://github.com/mila-iqia/climate-cooperation-competition/blob/)\n",
    "\n",
    "As an example, [here](https://github.com/mila-iqia/climate-cooperation-competition/blob/main/rice.py#L428) are the features added when the naive bilateral negotiation protocol is enabled in the simulator: \n",
    "\n",
    "``` python\n",
    "        if self.negotiation_on:\n",
    "            global_features += [\"stage\"]\n",
    "\n",
    "            public_features += []\n",
    "\n",
    "            private_features += [\n",
    "                \"minimum_mitigation_rate_all_regions\",\n",
    "            ]\n",
    "\n",
    "            bilateral_features += [\n",
    "                \"promised_mitigation_rate\",\n",
    "                \"requested_mitigation_rate\",\n",
    "                \"proposal_decisions\",\n",
    "            ]\n",
    "\n",
    "        shared_features = np.array([])\n",
    "        for feature in global_features + public_features:\n",
    "            shared_features = np.append(\n",
    "                shared_features,\n",
    "                self.flatten_array(\n",
    "                    self.global_state[feature][\"value\"][self.timestep]\n",
    "                    / self.global_state[feature][\"norm\"]\n",
    "                ),\n",
    "            )\n",
    "\n",
    "\n",
    "```\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "l1D5ZPREoPLG"
   },
   "source": [
    "## How to add actions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_zdRJ2-yqlAq"
   },
   "source": [
    "By default, agents' actions are contained in [`self.actions_nvec`](https://github.com/mila-iqia/climate-cooperation-competition/blob/main/rice.py#L136) during [`init()`](https://github.com/mila-iqia/climate-cooperation-competition/blob/main/rice.py#L64):\n",
    "\n",
    "```python\n",
    "        self.actions_nvec = (\n",
    "            self.savings_action_nvec\n",
    "            + self.mitigation_rate_action_nvec\n",
    "            + self.export_action_nvec\n",
    "            + self.import_actions_nvec\n",
    "            + self.tariff_actions_nvec\n",
    "        )\n",
    "\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "M-lrkKZqoRKM"
   },
   "source": [
    "Extra actions related to the negotiation protocol can be appended to `self.actions_nvec`.\n",
    "It is important that extra actions be appended at the **end** of `self.actions_nvec`.\n",
    "``` python \n",
    "            # Each region proposes to each other region\n",
    "            # self mitigation and their mitigation values\n",
    "            self.proposal_actions_nvec = (\n",
    "                [self.num_discrete_action_levels] * 2 * self.num_regions\n",
    "            )\n",
    "\n",
    "            # Each region evaluates a proposal from every other region,\n",
    "            # either accept or reject.\n",
    "            self.evaluation_actions_nvec = [2] * self.num_regions\n",
    "\n",
    "            # extra actions are appended to the end of self.actions_nvec\n",
    "            self.actions_nvec += (\n",
    "                self.proposal_actions_nvec + self.evaluation_actions_nvec\n",
    "            )\n",
    "\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "FjAU4P1elHS7"
   },
   "source": [
    "## How to implement the logic for negotiation protocols"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "7JwolIYnlHS7"
   },
   "source": [
    "The baseline logic for bilateral negotiation actions is a naive bargain process with two steps:\n",
    "1. A [``proposal_step()``](https://github.com/mila-iqia/climate-cooperation-competition/blob/main/rice.py#L536) for each agent to propose certains actions to other agents, for example a minimum mitigation rate.\n",
    "2. An [``evaluation_step()``](https://github.com/mila-iqia/climate-cooperation-competition/blob/main/rice.py#L585) for each agent to evaluation other agents' proposals. \n",
    "\n",
    "These functions describe how the negotiations actions affect the observation space and the action masking (for more, see the next section).\n",
    "Both steps are done sequentially in the [``step()``](https://github.com/mila-iqia/climate-cooperation-competition/blob/main/rice.py#L346) function in [``rice.py``](https://github.com/mila-iqia/climate-cooperation-competition/blob/main/rice.py): \n",
    "\n",
    "```python\n",
    "        if self.negotiation_on:\n",
    "            # Note: The '+1` below is for the climate_and_economy_simulation_step\n",
    "            self.stage = self.timestep % (self.num_negotiation_stages + 1)\n",
    "            self.set_global_state(\n",
    "                \"stage\", self.stage, self.timestep, dtype=self.int_dtype\n",
    "            )\n",
    "            if self.stage == 1:\n",
    "                return self.proposal_step(actions)\n",
    "\n",
    "            if self.stage == 2:\n",
    "                return self.evaluation_step(actions)\n",
    "\n",
    "        return self.climate_and_economy_simulation_step(actions)\n",
    "\n",
    "```\n",
    "Once the stages of the negotiation protocol are concluded, then the [`climate_and_economy_simulation_step()`](https://github.com/mila-iqia/climate-cooperation-competition/blob/main/rice.py#L651) implements the socioeconomic and climate dynamics associated with the updated observation space and masked actions.\n",
    "\n",
    "We expect competitors to propose different mechanisms to encourage global cooperation along climate and economic objectives.\n",
    "Participants should therefore modify this code to match the logic of their proposed negotiation protocol, even proposing new functions to replace [``proposal_step()``](https://github.com/mila-iqia/climate-cooperation-competition/blob/main/rice.py#L536), [``evaluation_step()``](https://github.com/mila-iqia/climate-cooperation-competition/blob/main/rice.py#L585) and the code above.\n",
    "\n",
    "For example, competitors could propose a mechanism to form [dynamic climate clubs](https://williamnordhaus.com/publications/climate-clubs-overcoming-free-riding-international-climate-policy), where admittance is based on a minimum mitigation rate. Club members enjoy lower tariffs when trading with other club members, while non-members, who do not have to contribute to mitigation, suffer heavy tariffs when trading with club members.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "A1OGL7JAlHS7"
   },
   "source": [
    "## What is masking?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "3ww_LsgvlHS7"
   },
   "source": [
    "Action masking determines the feasible subspace of the action space according to the negotiation protocol. Action masks are set before agents choose their actions, so the agent explicitly chooses from the feasible action subspace.\n",
    "To implement this logic, actions masks are modified in the [``evaluation_step()``](https://github.com/mila-iqia/climate-cooperation-competition/blob/main/rice.py#L585), after the [``proposal_step()``](https://github.com/mila-iqia/climate-cooperation-competition/blob/main/rice.py#L536) and [``evaluation_step()``](https://github.com/mila-iqia/climate-cooperation-competition/blob/main/rice.py#L585), but before the [`climate_and_economy_simulation_step()`](https://github.com/mila-iqia/climate-cooperation-competition/blob/main/rice.py#L651). This way, the regions are prohibited from taking actions outside of the feasible action subspace.\n",
    "\n",
    "For example, during the bilateral negotiation process, regions that agree to implement minimum mitigation rates are required to do so. \n",
    "\n",
    "```python\n",
    "        for region_id in range(self.num_regions):\n",
    "            outgoing_accepted_mitigation_rates = [\n",
    "                self.global_state[\"promised_mitigation_rate\"][\"value\"][\n",
    "                    self.timestep, region_id, j\n",
    "                ]\n",
    "                * self.global_state[\"proposal_decisions\"][\"value\"][\n",
    "                    self.timestep, j, region_id\n",
    "                ]\n",
    "                for j in range(self.num_regions)\n",
    "            ]\n",
    "            incoming_accepted_mitigation_rates = [\n",
    "                self.global_state[\"requested_mitigation_rate\"][\"value\"][\n",
    "                    self.timestep, j, region_id\n",
    "                ]\n",
    "                * self.global_state[\"proposal_decisions\"][\"value\"][\n",
    "                    self.timestep, region_id, j\n",
    "                ]\n",
    "                for j in range(self.num_regions)\n",
    "            ]\n",
    "\n",
    "            self.global_state[\"minimum_mitigation_rate_all_regions\"][\"value\"][\n",
    "                self.timestep, region_id\n",
    "            ] = max(\n",
    "                outgoing_accepted_mitigation_rates + incoming_accepted_mitigation_rates\n",
    "            )\n",
    "\n",
    "```\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "G37-_TDLlHS7"
   },
   "source": [
    "## How to implement and/or modify the logic of action masking?\n",
    "\n",
    "The logic behind action masks is implemented in [`generate_action_mask()`](https://github.com/mila-iqia/climate-cooperation-competition/blob/main/rice.py#L506).\n",
    "`mask_dict` gives the mapping for each region to its corresponding action `mask`. In the current implementation, `mask` is a binary vector where `0` indicates an action that is not allowed, and `1` indicates an action that is allowed.\n",
    "\n",
    "For example, in the bilateral negotiation protocol, the action mask is based on the minimum mitigation rate for each region (see code below).\n",
    "```python\n",
    "    def generate_action_mask(self):\n",
    "        \"\"\"\n",
    "        Generate action masks.\n",
    "        \"\"\"\n",
    "        mask_dict = {region_id: None for region_id in range(self.num_regions)}\n",
    "        for region_id in range(self.num_regions):\n",
    "            mask = self.default_agent_action_mask.copy()\n",
    "            if self.negotiation_on:\n",
    "                minimum_mitigation_rate = int(round(\n",
    "                    self.global_state[\"minimum_mitigation_rate_all_regions\"][\"value\"][\n",
    "                        self.timestep, region_id\n",
    "                    ]\n",
    "                    * self.num_discrete_action_levels\n",
    "                ))\n",
    "                mitigation_mask = np.array(\n",
    "                    [0 for _ in range(minimum_mitigation_rate)]\n",
    "                    + [\n",
    "                        1\n",
    "                        for _ in range(\n",
    "                            self.num_discrete_action_levels - minimum_mitigation_rate\n",
    "                        )\n",
    "                    ]\n",
    "                )\n",
    "                mask_start = sum(self.savings_action_nvec)\n",
    "                mask_end = mask_start + sum(self.mitigation_rate_action_nvec)\n",
    "                mask[mask_start:mask_end] = mitigation_mask\n",
    "            mask_dict[region_id] = mask\n",
    "\n",
    "        return mask_dict\n",
    "\n",
    "```"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [
    "ytMyQ2OHlHSr",
    "ukp1MeR1Q0dG",
    "4BIL2upxlHSw",
    "hrqSp18wlHS2",
    "OF4mXVOHlHS3",
    "0BCG5IYWlHS5",
    "yDI4p7cqlHS6",
    "LuF76W4FlHS6"
   ],
   "name": "Copy of Colab_Tutorial.ipynb",
   "provenance": []
  },
  "gpuClass": "standard",
  "kernelspec": {
   "display_name": "bleeding",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  },
  "vscode": {
   "interpreter": {
    "hash": "218b0c05efe39adc3475e81f74ab2f89e727d5ce721a75d4464da6cae91227c9"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
