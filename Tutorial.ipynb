{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Copyright (c) 2022, salesforce.com, inc and MILA.  \n",
    "All rights reserved.  \n",
    "SPDX-License-Identifier: BSD-3-Clause  \n",
    "For full license text, see the LICENSE file in the repo root  \n",
    "or https://opensource.org/licenses/BSD-3-Clause  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Install required packages"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-06-13T13:41:49.356590Z",
     "start_time": "2022-06-13T13:41:22.620490Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "!pip install rl_warp_drive\n",
    "!pip install rllib\n",
    "!pip install matplotlib\n",
    "!pip install -r requirements.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\", category=FutureWarning)\n",
    "\n",
    "sys.path.append(os.getcwd()+\"/scripts\")\n",
    "sys.path = [os.getcwd()+\"/scripts\"] + sys.path\n",
    "\n",
    "from desired_outputs import desired_outputs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train agents with GPU"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To train with GPU, you need to make sure that you have an **Nvdia Graphic Card** and be able to install critical packages such as ``warp-drive`` and ``pytorch``. If you don't have an Nvdia Graphic Card, you may refer to the section **Train Agents with CPU** below.\n",
    "\n",
    "In a word, to install ``warp-drive``, one can run ``pip install rl_warp_drive``. If errors pop out, please check [here](https://github.com/salesforce/warp-drive) for more details.\n",
    "\n",
    "To install pytorch with support of CUDA, a quick trial would be ``conda install pytorch torchvision torchaudio cudatoolkit=10.2 -c pytorch`` if one runs a conda virtual environment. For more details, please refer to [here](https://pytorch.org/get-started/locally/).\n",
    "\n",
    "If you encounter this error, please try to reduce your ``train_batch_size`` or ``num_envs``.\n",
    "\n",
    "```\n",
    "RuntimeError: CUDA out of memory. Tried to allocate\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gpu_trainer import trainer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To train the agents without naive negotiation and ensemble results with 100 random intialized enviornments and 1024 batch size. This training process is done by a single GPU.\n",
    "\n",
    "```python\n",
    "negotiation_on = 0 # without naive negotiation\n",
    "num_envs = 100 # ensemble results with 100 random intialized enviornments\n",
    "train_batch_size = 1024 # train with 1024 batch_size\n",
    "num_episodes = 30000 # number of episodes\n",
    "lr = 0.005 # learning rate\n",
    "model_params_save_freq = 5000 # save model for every 5000 steps\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gpu_trainer_off, gpu_nego_off_ts = trainer(negotiation_on=0, num_envs=100, train_batch_size=1024, num_episodes=30000, lr=0.0005, model_params_save_freq=5000, desired_outputs=desired_outputs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To train the agents with naive action and ensemble results with 100 random intialized enviornments and 1024 batch size. This training process is done by a single GPU.\n",
    "\n",
    "```python\n",
    "negotiation_on = 1 # with naive negotiation\n",
    "num_envs = 100 # ensemble results with 100 random intialized enviornments\n",
    "train_batch_size = 1024 # train with 1024 batch_size\n",
    "num_episodes = 30000 # number of episodes\n",
    "lr = 0.005 # learning rate\n",
    "model_params_save_freq = 5000 # save model for every 5000 steps\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "gpu_trainer_on, gpu_nego_on_ts = trainer(negotiation_on=1, num_envs=100, train_batch_size=1024, num_episodes=30000, lr=0.0005, model_params_save_freq=5000, desired_outputs=desired_outputs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To customize the training script, please check ``gpu_trainer.py`` for more details."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train agents with CPU"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To train agents with CPU, if the process is killed, one probably need to reduce ``num_envs`` and ``train_batch_size``. One should also expected longer period to train agents. Besides, please notice that training with negotiation usually need **3x** computational resource than training without negotiation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from cpu_trainer import trainer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This is necessary for rllib to get the correct path!\n",
    "os.chdir(os.getcwd()+\"/scripts\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To train the agents with naive actions and ensemble results with 100 random intialized enviornments and 1024 batch size. This training process is done by a single CPU (``num_workers=1``).\n",
    "\n",
    "```python\n",
    "negotiation_on = 0 # with naive negotiation\n",
    "num_envs = 1 # ensemble results with 100 random intialized enviornments\n",
    "train_batch_size = 1024 # train with 1024 batch_size\n",
    "num_episodes = 30000 # number of episodes\n",
    "lr = 0.005 # learning rate\n",
    "model_params_save_freq = 5000 # save model for every 5000 steps\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "cpu_trainer_off, cpu_nego_off_ts = trainer(negotiation_on=0, num_envs=1, train_batch_size=1024, num_episodes=300, lr=0.0005, model_params_save_freq=5000, desired_outputs=desired_outputs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To train the agents with naive actions and ensemble results with 100 random intialized enviornments and 1024 batch size. This training process is done by a single CPU (``num_workers=1``).\n",
    "\n",
    "```python\n",
    "negotiation_on = 1 # with naive negotiation\n",
    "num_envs = 1 # ensemble results with 100 random intialized enviornments\n",
    "train_batch_size = 1024 # train with 1024 batch_size\n",
    "num_episodes = 30000 # number of episodes\n",
    "lr = 0.005 # learning rate\n",
    "model_params_save_freq = 5000 # save model for every 5000 steps\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cpu_trainer_on, cpu_nego_on_ts = trainer(negotiation_on=1, num_envs=1, train_batch_size=1024, num_episodes=300, lr=0.0005, model_params_save_freq=5000, desired_outputs=desired_outputs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Save or load from previous training results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This section is for saving and loading the results (not the trainer) which is based on ``pickle``"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from opt_helper import save, load"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To save the output timeseries, one can do:\n",
    "```python\n",
    "save({\"nego_off\":nego_off_ts, \"nego_on\":nego_on_ts}, \"filename.pkl\")\n",
    "```\n",
    "\n",
    "To load the output timeseries, one can do:\n",
    "```python\n",
    "dict_ts = load(\"filename.pkl\")\n",
    "nego_off_ts, nego_on_ts = dict_ts[\"nego_off\"], dict_ts[\"nego_on\"]\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# [uncomment the below to save]\n",
    "# save({\"nego_off\":nego_off_ts, \"nego_on\":nego_on_ts}, \"filename.pkl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# [uncomment the below to load]\n",
    "dict_ts = load(\"filename.pkl\")\n",
    "nego_off_ts, nego_on_ts = dict_ts[\"nego_off\"], dict_ts[\"nego_on\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The available data that we can plot"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Plot results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from desired_outputs import desired_outputs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "One may want to check the performance of the agents by plotting graphs. Below, we list all the logged variables. One may change the ``desired_outputs.py`` to add more variables of interest.\n",
    "\n",
    "```python\n",
    "desired_outputs = ['global_temperature', 'global_carbon_mass', 'capital_all_regions', 'labor_all_regions', 'production_factor_all_regions', 'intensity_all_regions', 'global_exogenous_emissions', 'global_land_emissions', 'timestep', 'activity_timestep', 'capital_depreciation_all_regions', 'savings_all_regions', 'mitigation_rate_all_regions', 'max_export_limit_all_regions', 'mitigation_cost_all_regions', 'damages_all_regions', 'abatement_cost_all_regions', 'utility_all_regions', 'social_welfare_all_regions', 'reward_all_regions', 'consumption_all_regions', 'current_balance_all_regions', 'gross_output_all_regions', 'investment_all_regions', 'production_all_regions', 'tariffs', 'future_tariffs', 'scaled_imports', 'desired_imports', 'tariffed_imports', 'stage', 'minimum_mitigation_rate_all_regions', 'promised_mitigation_rate', 'requested_mitigation_rate', 'proposal_decisions']\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from opt_helper import plot_result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The plot_result function plots the time series of all the logged variables.\n",
    "\n",
    "```python\n",
    "plot_result(variables, nego_off_ts, nego_on_ts, k)\n",
    "```\n",
    "``variables`` can be either a list of variable names comes from the above list or a single variable of interest. The ``nego_off_ts`` and ``nego_on_ts`` are the time series loggings for these variables. ``k`` represents the dimension of the interest data, for most of situation, it should be ``0`` by default."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_result(desired_outputs, nego_off_ts, nego_on_ts, k=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_result(\"global_temperature\", nego_off_ts, nego_on_ts, k=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# How to quickly evaluate the results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This section to for evaluating the trained agents. One can edit the evaluate function ``eval metrics`` here ``evaluate_submission.py`` if interested in more metrics.\n",
    "\n",
    "To use the evaluation script, one need to input the trainer, logged_variables and the framework of the trainer.\n",
    "The first 2 are given by the ``trainer`` function as above. If one train the agents with GPU, then the framework should be ``warpdrive``. If one train the framework using CPU, it should be ``rllib``.\n",
    "\n",
    "We give one example below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from evaluate_submission import val_metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "val_metrics(trainer=gpu_trainer_off, logged_ts=gpu_nego_off_ts, framework=\"warpdrive\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# How to modify the simulation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introduction of environment code"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "``rice.py``, ``rice_cuda.py``, ``rice_step.cu`` and ``rice_helpers.py`` are responsible for the GPU code. \n",
    "\n",
    "Among them, ``rice_helpers.py`` includes all the social-economics-climate dynamics and this files should not be changed.\n",
    "\n",
    "``rice.py`` includes the patterns of agents interact with the environment, which should be the main script to be modified.\n",
    "\n",
    "[GPU needed] ``rice_cuda.py`` connects the data between the python script and CUDA code.\n",
    "\n",
    "[GPU needed] ``rice_step.cu`` includes the CUDA version codes of both the social-economics-climate dynamics and the patterns of agents interact with the environment. To train the agent with GPU, the CUDA code must share the same logic with the python codes. The CUDA code mostly follows the grammar of C++. Please refer to [here](https://docs.nvidia.com/cuda/cuda-c-programming-guide/index.html) for more details."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## How to add extra observations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To add extra observations, one need to add the initiation of the observations in the `rest()` and `generate_observation()` function in `rice.py`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## How to change the logic of taking actions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The baseline logic of taking actions are a naive bargain process including a ``proposal_step()`` for each agent to propose the next step and a ``evaluation_step()`` for each agent to evaluation others proposal and determine  the tariff and international trade volumes. They are fulfilled in the ``step()`` function in the ``rice.py``.\n",
    "\n",
    "We expect competitors are able to propose a mechanism to form a [dynamic climate club](https://williamnordhaus.com/publications/climate-clubs-overcoming-free-riding-international-climate-policy) so that countries in the club may enjoy more trades and less tariff while those who contribute less on the climate mitigation might suffer more tariff and less trades."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## What is masking?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TBC"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## How to modify your masking?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TBC"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
