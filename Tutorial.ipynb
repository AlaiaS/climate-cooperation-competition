{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Copyright (c) 2022, salesforce.com, inc and MILA.  \n",
    "All rights reserved.  \n",
    "SPDX-License-Identifier: BSD-3-Clause  \n",
    "For full license text, see the LICENSE file in the repo root  \n",
    "or https://opensource.org/licenses/BSD-3-Clause  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# How can I register for the competition?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Please fill out the [registration form](https://docs.google.com/forms/d/e/1FAIpQLSe2SWnhJaRpjcCa3idq7zIFubRoH0pATLOP7c1Y0kMXOV6U4w/viewform) in order to register for the competition. \n",
    "\n",
    "You will only need to provide an email address and a team name. You will also need to be willing to open-source your code after the competition.\n",
    "\n",
    "After you submit your registration form, we will register it internally. Please allow for upto 1-2 working days for your team name to be registered. You will be notified via email upon successful registration. You will need your team name in order to make submissions towards the competition."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Getting started"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Quickly train agents with CPU using rllib and create a submission"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following command should install all the pre-requisites automatically.\n",
    "\n",
    "Please make sure that you are using Python 3.7 or older version. Our code does not support 3.8 or newer versions currently."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!python ./scripts/train_with_rllib.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluate your submission locally"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before you actually upload your submission files, you can also evaluate and score your submission on your end using this script. The evaluation script essentially validates the submission files, performs unit testing and computes the metrics for evaluation. To compute the metrics, we first instantiate a trainer, load the policy model with the saved parameters, and then generate several episode rollouts to measure the impact of the policy on the environment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!python ./scripts/evaluate_submission.py -r Submissions/<submission_number>.zip"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Where can I submit my solution?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*NOTE: Please register for the competition (see the steps above), if you have not done so. Your team must be registered before you can submit your solutions.*\n",
    "\n",
    "The AI climate competition features 3 tracks.\n",
    "\n",
    "In Track 1, you will propose and implement multilateral agreements to augment the simulator, and train the AI agents in the simulator. We evaluate the learned policies and resulting economic and climate change metrics.\n",
    "\n",
    "- The submission form for Track 1 is [here](https://docs.google.com/forms/d/e/1FAIpQLSdATpPMnhjXNFAnGNRU2kuufwD5HFilGxgIXFK9QKsqrDbkog/viewform).\n",
    "\n",
    "Please select your registered team name from the drop-down menu, and upload a zip file containing the submission files - we will be providing scripts to help you create the zip file.\n",
    "\n",
    "In Track 2, you will argue why your solution is practically relevant and usable in the real world. We expect the entries in this track to contain a high-level summary for policymakers\n",
    "\n",
    "- The submission form for Track 2 is [here](https://docs.google.com/forms/d/e/1FAIpQLSeoc4oLBU4c8EoumkocSyhRaxGW0JoEVcBgeuo-U9fSfNOyrQ/viewform).\n",
    "\n",
    "\n",
    "In Track 3, we invite you to point out potential simulation loopholes and improvements.\n",
    "\n",
    "- The submission form for Track 3 is [here](https://docs.google.com/forms/d/e/1FAIpQLSed0seSYt8LKywVrE7BARxAPPsO6WYmPUMeIezD7FTV176QvQ/viewform).\n",
    "\n",
    "If you do not see your team name in the drop-down menu, please contact us on Slack or by e-mail, and we will resolve that for you."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# How do I create a submission using my modified negotiation protocol? "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We provide the base version of the RICE-N (regional integrated climate environment) simulation environment written in Python (`rice.py`).\n",
    "\n",
    "**For the mathematical background and scientific references, please see [the white paper](https://github.com/mila-iqia/climate-cooperation-competition/blob/website/website/src/pdf/ai-for-global-climate-cooperation-competition_white-paper.pdf).**\n",
    "\n",
    "You will need to mainly modify the `rice.py` to implement the proposed negotiatoin protocol. Additional details can be found below."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Scripts for creating the zipped submission file"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As mentioned above, the zipped file required for submission is automatically created post-training. However, for any reason (for example, for providing a trained policy model at a different timestep), you can create the zipped submission yourself using the `create_submizzion_zip.py` script. Accordingly, create a new directory (say `submission_dir`) with all the relevant files (see the section above), and you can then simply invoke\n",
    "```commandline\n",
    "python scripts/create_submission_zip.py -r <PATH-TO-SUBMISSION-DIR>\n",
    "```\n",
    "\n",
    "That will first validate that the submission directory contains all the required files, and then provide you a zipped file that can you use towards your submission."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!python ./scripts/create_submission_zip.py -r <PATH-TO-SUBMISSION-DIR>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Scripts for unit testing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In order to make sure that all the submissions are consistent in that they comply within the rules of the competition, we have also added unit tests. These are automatically run also when the evaluation is performed. The script currently performs the following tests\n",
    "\n",
    "- Test that the environment attributes (such as the RICE and DICE constants, the simulation period and the number of regions) are consistent with the base environment class that we also provide.\n",
    "- Test that the `climate_and_economy_simulation_step()` is consistent with the base class. As aforementioned, users are free to add different negotiation strategies such as multi-lateral negotiations or climate clubs, but should not modify the equations underlying the climate and economic dynamics in the world.\n",
    "- Test that the environment resetting and stepping yield outputs in the desired format (for instance, observations are a dictionary keyed by region id, and so are rewards.)\n",
    "- If the user used WarpDrive, we also perform consistency checks to verify that the CUDA implementation of the rice environment is consistent with the pythonic version.\n",
    "\n",
    "USAGE: You may invoke the unit tests on a submission file via\n",
    "```commandline\n",
    "python scripts/run_unittests.py -r <PATH-TO-ZIP-FILE>\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!python ./scripts/run_unittests.py -r <PATH-TO-ZIP-FILE>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Scripts for performance evaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "USAGE: You may evaluate the submission file using\n",
    "```commandline\n",
    "python scripts/evaluate_submission.py -r <PATH-TO-ZIP-FILE>\n",
    "```\n",
    "Please verify that you can indeed evaluate your submission, before actually uploading it."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluation process"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After you submit your solution, we will be using the same evaluation script that is provided to you, to score your submissions, but using several rollout episodes to average the metrics such as the average rewards, the global temperature rise, capital, production, and many more. We will then rank the submissions based on the various metrics.The score computed by the evaluation process should be similar to the score computed on your end, since they use the same scripts."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## What happens when I make an invalid submission?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "An \"invalid submission\" may refer to a submission wherein some or all of the submission files are missing, or the submission files are inconsistent with the base version, basically anything that fails in the evaluation process. Any invalid solution cannot be evaluated, and hence will not feature in the leaderboard. While we can let you know if your submission is invalid, the process is not automated, so we may not be able to do it promptly. To avoid any issues, please use the `create_submission_zip` script to create your zipped submission file."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Leaderboard"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The competition leaderboard is displayed on the [competition website](https://mila-iqia.github.io/climate-cooperation-competition). After you submit your valid submission, please give it a few minutes to perform an evaluation of your submission and refresh the leaderboard."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# How many submissions are allowed per team?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There is no limit on the number of submissions per team. Feel free to submit as many solutions as you would like. We will only be using your submission with the highest evaluation score towards the leaderboard."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-06-13T13:41:49.356590Z",
     "start_time": "2022-06-13T13:41:22.620490Z"
    },
    "scrolled": true
   },
   "source": [
    "# Code overview"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## File Structure"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Below is the detailed file tree, and file descriptions.\n",
    "```commandline\n",
    "ROOT_DIR\n",
    "├── rice.py\n",
    "├── rice_helpers.py\n",
    "├── region_yamls\n",
    "\n",
    "├── rice_step.cu\n",
    "├── rice_cuda.py\n",
    "├── rice_build.cu\n",
    "\n",
    "└── scripts\n",
    "    ├── train_with_rllib.py\n",
    "    ├── rice_rllib.yaml\n",
    "    ├── torch_models.py\n",
    "    \n",
    "    ├── train_with_warp_drive.py\n",
    "    ├── rice_warpdrive.yaml\n",
    "    ├── run_cpu_gpu_env_consistency_checks.py\n",
    "    \n",
    "    ├── run_unittests.py    \n",
    "    ├── create_submission_zip.py\n",
    "    └── evaluate_submission.py   \n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Environment files"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- `rice.py`: This python script contains the base Rice class. This is written in [OpenAI Gym](https://gym.openai.com/) style with the `reset()` and `step()` functionalities. The step() function comprises an implementation of the `climate_and_economy_simulation_step` which dictate the dynamics of the climate and economy simulation, and should not be altered by the user. We have also provided a simple implementation of bilateral negotiation between regions via the `proposal_step()` and `evaluation_step()` methods. Users can extend the simulation by adding additional proposal strategies, for example, and incorporating them in the `step()` function. However, please do not modify any of the equations dictating the environment dynamics in the `climate_and_economy_simulation_step()`. All the helper functions related to modeling the climate and economic simulation are located in `rice_helpers.py`. Region-specific environment parameters are provided in the `region_yamls` directory.\n",
    "\n",
    "\n",
    "- `rice_step.cu`\n",
    "This is the CUDA C version of the step() function that is required for use with WarpDrive. To get started with WarpDrive, we recommend following these [tutorials](https://github.com/salesforce/warp-drive/tree/master/tutorials). While WarpDrive requires writing the simulation in CUDA C, it also offers orders-of-magnitude speedups for end-to-end training, since it performs rollouts and training all on the GPU. `rice_cuda.py` nd `rice_build.cu` are necessary files for copying simulation data to the GPU and compiling the CUDA code.\n",
    "\n",
    "While implementing the simulation in CUDA C on the GPU offers significantly faster simulations, it requires careful memory management. To make sure that everything works properly, one approach is to first implement your simulation logic in Python. You can then implement the same logic in CUDA C and check the simulation behaviors are the same. To help with this process, we provide an environment consistency checker method to do consistency tests between Python and CUDA C simulations. Before training your CUDA C code, please run the consistency checker to ensure the Python and CUDA C implementations are consistent.\n",
    "```commandline\n",
    "python scripts/run_env_cpu_gpu_consistency_checks.py\n",
    "```\n",
    "\n",
    "See Training with GPU below for more on training with WarpDrive."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Install required packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-06-13T13:41:49.356590Z",
     "start_time": "2022-06-13T13:41:22.620490Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "!pip install rl_warp_drive\n",
    "!pip install rllib\n",
    "!pip install matplotlib\n",
    "!pip install -r requirements.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\", category=FutureWarning)\n",
    "\n",
    "sys.path.append(os.getcwd()+\"/scripts\")\n",
    "sys.path = [os.getcwd()+\"/scripts\"] + sys.path\n",
    "\n",
    "from desired_outputs import desired_outputs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train agents with GPU"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To train with GPU, you need to make sure that you have an **Nvdia Graphic Card** and be able to install critical packages such as ``warp-drive`` and ``pytorch``. If you don't have an Nvdia Graphic Card, you may refer to the section **Train Agents with CPU** below.\n",
    "\n",
    "In a word, to install ``warp-drive``, one can run ``pip install rl_warp_drive``. If errors pop out, please check [here](https://github.com/salesforce/warp-drive) for more details.\n",
    "\n",
    "To install pytorch with support of CUDA, a quick trial would be ``conda install pytorch torchvision torchaudio cudatoolkit=10.2 -c pytorch`` if one runs a conda virtual environment. For more details, please refer to [here](https://pytorch.org/get-started/locally/).\n",
    "\n",
    "If you encounter this error, please try to reduce your ``train_batch_size`` or ``num_envs``.\n",
    "\n",
    "```\n",
    "RuntimeError: CUDA out of memory. Tried to allocate\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gpu_trainer import trainer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To train the agents without naive negotiation and ensemble results with 100 random intialized enviornments and 1024 batch size. This training process is done by a single GPU.\n",
    "\n",
    "```python\n",
    "negotiation_on = 0 # without naive negotiation\n",
    "num_envs = 100 # ensemble results with 100 random intialized enviornments\n",
    "train_batch_size = 1024 # train with 1024 batch_size\n",
    "num_episodes = 30000 # number of episodes\n",
    "lr = 0.005 # learning rate\n",
    "model_params_save_freq = 5000 # save model for every 5000 steps\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gpu_trainer_off, gpu_nego_off_ts = trainer(negotiation_on=0, num_envs=100, train_batch_size=1024, num_episodes=30000, lr=0.0005, model_params_save_freq=5000, desired_outputs=desired_outputs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To train the agents with naive action and ensemble results with 100 random intialized enviornments and 1024 batch size. This training process is done by a single GPU.\n",
    "\n",
    "```python\n",
    "negotiation_on = 1 # with naive negotiation\n",
    "num_envs = 100 # ensemble results with 100 random intialized enviornments\n",
    "train_batch_size = 1024 # train with 1024 batch_size\n",
    "num_episodes = 30000 # number of episodes\n",
    "lr = 0.005 # learning rate\n",
    "model_params_save_freq = 5000 # save model for every 5000 steps\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "gpu_trainer_on, gpu_nego_on_ts = trainer(negotiation_on=1, num_envs=100, train_batch_size=1024, num_episodes=30000, lr=0.0005, model_params_save_freq=5000, desired_outputs=desired_outputs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To customize the training script, please check ``gpu_trainer.py`` for more details."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train agents with CPU"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To train agents with CPU, if the process is killed, one probably need to reduce ``num_envs`` and ``train_batch_size``. One should also expected longer period to train agents. Besides, please notice that training with negotiation usually need **3x** computational resource than training without negotiation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from cpu_trainer import trainer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This is necessary for rllib to get the correct path!\n",
    "os.chdir(os.getcwd()+\"/scripts\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To train the agents with naive actions and ensemble results with 100 random intialized enviornments and 1024 batch size. This training process is done by a single CPU (``num_workers=1``).\n",
    "\n",
    "```python\n",
    "negotiation_on = 0 # with naive negotiation\n",
    "num_envs = 1 # ensemble results with 100 random intialized enviornments\n",
    "train_batch_size = 1024 # train with 1024 batch_size\n",
    "num_episodes = 30000 # number of episodes\n",
    "lr = 0.005 # learning rate\n",
    "model_params_save_freq = 5000 # save model for every 5000 steps\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "cpu_trainer_off, cpu_nego_off_ts = trainer(negotiation_on=0, num_envs=1, train_batch_size=1024, num_episodes=300, lr=0.0005, model_params_save_freq=5000, desired_outputs=desired_outputs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To train the agents with naive actions and ensemble results with 100 random intialized enviornments and 1024 batch size. This training process is done by a single CPU (``num_workers=1``).\n",
    "\n",
    "```python\n",
    "negotiation_on = 1 # with naive negotiation\n",
    "num_envs = 1 # ensemble results with 100 random intialized enviornments\n",
    "train_batch_size = 1024 # train with 1024 batch_size\n",
    "num_episodes = 30000 # number of episodes\n",
    "lr = 0.005 # learning rate\n",
    "model_params_save_freq = 5000 # save model for every 5000 steps\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cpu_trainer_on, cpu_nego_on_ts = trainer(negotiation_on=1, num_envs=1, train_batch_size=1024, num_episodes=300, lr=0.0005, model_params_save_freq=5000, desired_outputs=desired_outputs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Save or load from previous training results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This section is for saving and loading the results (not the trainer) which is based on ``pickle``"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from opt_helper import save, load"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To save the output timeseries, one can do:\n",
    "```python\n",
    "save({\"nego_off\":nego_off_ts, \"nego_on\":nego_on_ts}, \"filename.pkl\")\n",
    "```\n",
    "\n",
    "To load the output timeseries, one can do:\n",
    "```python\n",
    "dict_ts = load(\"filename.pkl\")\n",
    "nego_off_ts, nego_on_ts = dict_ts[\"nego_off\"], dict_ts[\"nego_on\"]\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# [uncomment the below to save]\n",
    "# save({\"nego_off\":nego_off_ts, \"nego_on\":nego_on_ts}, \"filename.pkl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# [uncomment the below to load]\n",
    "dict_ts = load(\"filename.pkl\")\n",
    "nego_off_ts, nego_on_ts = dict_ts[\"nego_off\"], dict_ts[\"nego_on\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The available data that we can plot"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Plot results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from desired_outputs import desired_outputs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "One may want to check the performance of the agents by plotting graphs. Below, we list all the logged variables. One may change the ``desired_outputs.py`` to add more variables of interest.\n",
    "\n",
    "```python\n",
    "desired_outputs = ['global_temperature', 'global_carbon_mass', 'capital_all_regions', 'labor_all_regions', 'production_factor_all_regions', 'intensity_all_regions', 'global_exogenous_emissions', 'global_land_emissions', 'timestep', 'activity_timestep', 'capital_depreciation_all_regions', 'savings_all_regions', 'mitigation_rate_all_regions', 'max_export_limit_all_regions', 'mitigation_cost_all_regions', 'damages_all_regions', 'abatement_cost_all_regions', 'utility_all_regions', 'social_welfare_all_regions', 'reward_all_regions', 'consumption_all_regions', 'current_balance_all_regions', 'gross_output_all_regions', 'investment_all_regions', 'production_all_regions', 'tariffs', 'future_tariffs', 'scaled_imports', 'desired_imports', 'tariffed_imports', 'stage', 'minimum_mitigation_rate_all_regions', 'promised_mitigation_rate', 'requested_mitigation_rate', 'proposal_decisions']\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from opt_helper import plot_result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The plot_result function plots the time series of all the logged variables.\n",
    "\n",
    "```python\n",
    "plot_result(variables, nego_off_ts, nego_on_ts, k)\n",
    "```\n",
    "``variables`` can be either a list of variable names comes from the above list or a single variable of interest. The ``nego_off_ts`` and ``nego_on_ts`` are the time series loggings for these variables. ``k`` represents the dimension of the interest data, for most of situation, it should be ``0`` by default."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_result(desired_outputs, nego_off_ts, nego_on_ts, k=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_result(\"global_temperature\", nego_off_ts, nego_on_ts, k=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# How to quickly evaluate the results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This section to for evaluating the trained agents. One can edit the evaluate function ``eval metrics`` here ``evaluate_submission.py`` if interested in more metrics.\n",
    "\n",
    "To use the evaluation script, one need to input the trainer, logged_variables and the framework of the trainer.\n",
    "The first 2 are given by the ``trainer`` function as above. If one train the agents with GPU, then the framework should be ``warpdrive``. If one train the framework using CPU, it should be ``rllib``.\n",
    "\n",
    "We give one example below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from evaluate_submission import val_metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "val_metrics(trainer=gpu_trainer_off, logged_ts=gpu_nego_off_ts, framework=\"warpdrive\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# How to modify the simulation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introduction of environment code"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "``rice.py``, ``rice_cuda.py``, ``rice_step.cu`` and ``rice_helpers.py`` are responsible for the GPU code. \n",
    "\n",
    "Among them, ``rice_helpers.py`` includes all the social-economics-climate dynamics and this files should not be changed.\n",
    "\n",
    "``rice.py`` includes the patterns of agents interact with the environment, which should be the main script to be modified.\n",
    "\n",
    "[GPU needed] ``rice_cuda.py`` connects the data between the python script and CUDA code.\n",
    "\n",
    "[GPU needed] ``rice_step.cu`` includes the CUDA version codes of both the social-economics-climate dynamics and the patterns of agents interact with the environment. To train the agent with GPU, the CUDA code must share the same logic with the python codes. The CUDA code mostly follows the grammar of C++. Please refer to [here](https://docs.nvidia.com/cuda/cuda-c-programming-guide/index.html) for more details."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## How to add extra observations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To add extra observations, one need to add the initiation of the observations in the `rest()` and `generate_observation()` function in `rice.py`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## How to change the logic of taking actions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The baseline logic of taking actions are a naive bargain process including a ``proposal_step()`` for each agent to propose the next step and a ``evaluation_step()`` for each agent to evaluation others proposal and determine  the tariff and international trade volumes. They are fulfilled in the ``step()`` function in the ``rice.py``.\n",
    "\n",
    "We expect competitors are able to propose a mechanism to form a [dynamic climate club](https://williamnordhaus.com/publications/climate-clubs-overcoming-free-riding-international-climate-policy) so that countries in the club may enjoy more trades and less tariff while those who contribute less on the climate mitigation might suffer more tariff and less trades."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## What is masking?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TBC"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## How to modify your masking?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TBC"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
